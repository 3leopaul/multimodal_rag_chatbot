{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "90098b97",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append(\"../src\")\n",
                "\n",
                "import ollama\n",
                "from torch import load\n",
                "\n",
                "import gradio as gr\n",
                "import time\n",
                "from functions import *"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "b1652277",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ProgressResponse(status='success', completed=None, total=None, digest=None)"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Pull the llama3.2-vision model from Ollama to ensure it is available locally\n",
                "ollama.pull('llama3.2-vision')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "e8a63ae6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pre-processed text and image content from the JSON files\n",
                "text_content_list = load_from_json('../data/processed_files/JSON/text_content.json')\n",
                "image_content_list = load_from_json('../data/processed_files/JSON/image_content.json')\n",
                "\n",
                "# Load the pre-computed text and image embeddings from the .pt files\n",
                "text_embeddings = load('../data/processed_files/embeddings/text_embeddings.pt', weights_only=True)\n",
                "image_embeddings = load('../data/processed_files/embeddings/image_embeddings.pt', weights_only=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "4a3cc3d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the main chat function to handle user messages and stream responses from Ollama\n",
                "def stream_chat(message, history):\n",
                "\n",
                "    # Retrieve relevant text and image context based on the user's message\n",
                "    text_results, image_results = context_retrieval(message[\"text\"], text_embeddings, image_embeddings, text_content_list, image_content_list)\n",
                "\n",
                "    # Construct a detailed prompt including the retrieved context for the LLM\n",
                "    prompt = construct_prompt(message[\"text\"], text_results, image_results)\n",
                "    \n",
                "    # Prepare the list of image paths, using only the top result as the model supports single-image input\n",
                "    images = [image_results[0][\"image_path\"]] if image_results else []\n",
                "    \n",
                "    # Append the formatted user prompt and image to the conversation history\n",
                "    history.append({\"role\": \"user\", \"content\": prompt, \"images\": images})\n",
                "    \n",
                "    # Initialize a streaming chat session with the Ollama model using the full history\n",
                "    stream = ollama.chat(\n",
                "        model='llama3.2-vision',\n",
                "        messages=history,  # Full chat history including the current user message\n",
                "        stream=True,\n",
                "        options={\n",
                "        'temperature': 0.6,          # Slight randomness helps break loops\n",
                "        'repeat_penalty': 1.15,      # <<<< THIS IS THE KEY FIX\n",
                "        'top_k': 40,\n",
                "        'top_p': 0.9\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    response_text = \"\"\n",
                "    for chunk in stream:\n",
                "        content = chunk['message']['content']\n",
                "        response_text += content\n",
                "        yield response_text  # Send the response incrementally to the UI\n",
                "\n",
                "    # Append the assistant's complete response to the conversation history\n",
                "    history.append({\"role\": \"assistant\", \"content\": response_text})\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "3baead31",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "* Running on local URL:  http://127.0.0.1:7862\n",
                        "* To create a public link, set `share=True` in `launch()`.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": []
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Create and launch the Gradio ChatInterface for the multimodal chatbot\n",
                "gr.ChatInterface(\n",
                "    fn=stream_chat,  # The function handling the chat\n",
                "    type=\"messages\",  # Using \"messages\" to enable chat-style conversation\n",
                "    examples=[{\"text\": \"According to this resume, what is Leo-Paul looking for?\"}, \n",
                "              {\"text\": \"What are the key projects Leo-Paul participated in?\"},\n",
                "              {\"text\": \"What are Leo-Paul's hobbies?\"},\n",
                "              {\"text\": \"How would you describe Leo-Paul based on the picture in the CV?\"}],  # Example inputs\n",
                "    multimodal=True,\n",
                ").launch()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
