{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90098b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from torch import load\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1652277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull model\n",
    "ollama.pull('llama3.2-vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a63ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article contents\n",
    "text_content_list = load_from_json('data/text_content.json')\n",
    "image_content_list = load_from_json('data/image_content.json')\n",
    "\n",
    "# load embeddings\n",
    "text_embeddings = load('data/text_embeddings.pt', weights_only=True)\n",
    "image_embeddings = load('data/image_embeddings.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the Ollama model\n",
    "def stream_chat(message, history):\n",
    "    \"\"\"\n",
    "    Streams the response from the Ollama model and sends it to the Gradio UI.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user input message.\n",
    "        history (list): A list of previous conversation messages.\n",
    "        \n",
    "    Yields:\n",
    "        str: The chatbot's response chunk by chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # context retrieval\n",
    "    text_results, image_results = context_retrieval(message[\"text\"], text_embeddings, image_embeddings, text_content_list, image_content_list)\n",
    "\n",
    "    # construct prompt\n",
    "    prompt = construct_prompt(message[\"text\"], text_results, image_results)\n",
    "    \n",
    "    # Append the user message to the conversation history\n",
    "    history.append({\"role\": \"user\", \"content\": prompt, \"images\": [image[\"image_path\"] for image in image_results]})\n",
    "    \n",
    "    # Initialize streaming from Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2-vision',\n",
    "        messages=history,  # Full chat history including the current user message\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response_text += content\n",
    "        yield response_text  # Send the response incrementally to the UI\n",
    "\n",
    "    # Append the assistant's full response to the history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baead31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio ChatInterface\n",
    "gr.ChatInterface(\n",
    "    fn=stream_chat,  # The function handling the chat\n",
    "    type=\"messages\",  # Using \"messages\" to enable chat-style conversation\n",
    "    examples=[{\"text\": \"What is CLIP's contrastive loss function?\"}, \n",
    "              {\"text\": \"What are the three paths described for making LLMs multimodal?\"},\n",
    "              {\"text\": \"What is an intuitive explanation of multimodal embeddings?\"}],  # Example inputs\n",
    "    multimodal=True,\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
