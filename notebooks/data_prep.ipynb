{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "46dbce5e",
            "metadata": {},
            "source": [
                "Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "98963958",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append(\"../src\")\n",
                "import os\n",
                "from functions import *\n",
                "from PIL import Image\n",
                "from transformers import CLIPProcessor, CLIPModel\n",
                "from torch import save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "c3fed56c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all PDF files from raw directory\n",
                "\n",
                "filename_list = [\"../data/raw_files/pdf/\"+f for f in os.listdir('../data/raw_files/pdf') if f.endswith('.pdf')]\n",
                "\n",
                "text_content_list = []\n",
                "image_content_list = []\n",
                "for filename in filename_list:\n",
                "    text_content_list.extend(parse_pdf_content(filename))\n",
                "    image_content_list.extend(parse_pdf_images(filename))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "9c01ab2e",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(len(text_content_list))\n",
                "print(len(image_content_list))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "0f40cf31",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import processor first to use for tokenization\n",
                "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
                "\n",
                "text_list = []\n",
                "for content in text_content_list:\n",
                "    # concatenate title and section header\n",
                "    section = content['section'] + \": \"\n",
                "    full_text = section + content['text']\n",
                "    \n",
                "    # Tokenize and truncate to CLIP's max length (77 tokens)\n",
                "    tokens = processor.tokenizer(full_text, truncation=True, max_length=77, return_tensors=\"pt\")\n",
                "    # Decode back to text to ensure consistency\n",
                "    truncated_text = processor.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
                "    \n",
                "    text_list.append(truncated_text)\n",
                "\n",
                "image_list = []\n",
                "for content in image_content_list:\n",
                "    image_list.append(Image.open(content['image_path']))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "5d714731",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(len(text_list))\n",
                "print(len(image_list))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "091e0550",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import model\n",
                "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "41d20512",
            "metadata": {},
            "outputs": [],
            "source": [
                "# pre-process text and images\n",
                "inputs = processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3eaf452",
            "metadata": {},
            "outputs": [],
            "source": [
                "# compute embeddings with CLIP\n",
                "outputs = model(**inputs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "93200dad",
            "metadata": {},
            "outputs": [],
            "source": [
                "# store embeddings in single torch tensor\n",
                "text_embeddings = outputs.text_embeds\n",
                "image_embeddings = outputs.image_embeds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd76d7ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(text_embeddings.shape)\n",
                "print(image_embeddings.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "90bc80db",
            "metadata": {},
            "outputs": [],
            "source": [
                "# save content list as JSON\n",
                "save_to_json(text_content_list, output_file='../data/processed_files/JSON/text_content.json')\n",
                "save_to_json(image_content_list, output_file='../data/processed_files/JSON/image_content.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f667c1a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# save embeddings to file\n",
                "save(text_embeddings, '../data/processed_files/embeddings/text_embeddings.pt')\n",
                "save(image_embeddings, '../data/processed_files/embeddings/image_embeddings.pt')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}