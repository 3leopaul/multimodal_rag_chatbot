{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "46dbce5e",
            "metadata": {},
            "source": [
                "Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "98963958",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append(\"../src\")\n",
                "import os\n",
                "from functions import *\n",
                "from PIL import Image\n",
                "from transformers import CLIPProcessor, CLIPModel\n",
                "from torch import save\n",
                "import shutil\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c3fed56c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove the 'processed_files' directory if it exists to ensure a clean start\n",
                "if os.path.exists('../data/processed_files/'):\n",
                "    shutil.rmtree('../data/processed_files/')\n",
                "\n",
                "# Create the necessary subdirectories for storing images, JSON data, and embeddings\n",
                "os.makedirs('../data/processed_files/extracted_images/', exist_ok=True)\n",
                "os.makedirs('../data/processed_files/JSON/', exist_ok=True)\n",
                "os.makedirs('../data/processed_files/embeddings/', exist_ok=True)\n",
                "\n",
                "# List all PDF files found in the raw data directory\n",
                "filename_list = [\"../data/raw_files/pdf/\"+f for f in os.listdir('../data/raw_files/pdf') if f.endswith('.pdf')]\n",
                "\n",
                "text_content_list = []\n",
                "image_content_list = []\n",
                "for filename in filename_list:\n",
                "    text_content_list.extend(parse_pdf_content(filename))\n",
                "    image_content_list.extend(parse_pdf_images(filename))\n",
                "# Extract text and images from all PDF files and save images to disk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9c01ab2e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "14\n",
                        "1\n"
                    ]
                }
            ],
            "source": [
                "print(len(text_content_list))\n",
                "print(len(image_content_list))\n",
                "\n",
                "# Print the total number of extracted text chunks and images for verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "0f40cf31",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the CLIP processor for tokenizing text content\n",
                "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
                "\n",
                "text_list = []\n",
                "for content in text_content_list:\n",
                "    # Combine the section header and text content into a single string\n",
                "    section = content['section'] + \": \"\n",
                "    full_text = section + content['text']\n",
                "    \n",
                "    # Tokenize the text and truncate it to the CLIP model's maximum length of 77 tokens\n",
                "    tokens = processor.tokenizer(full_text, truncation=True, max_length=77, return_tensors=\"pt\")\n",
                "    # Decode the tokens back to text to ensure consistency with the model's input format\n",
                "    truncated_text = processor.tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
                "    \n",
                "    text_list.append(truncated_text)\n",
                "\n",
                "image_list = []\n",
                "for content in image_content_list:\n",
                "    image_list.append(Image.open(content['image_path']))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "5d714731",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "14\n",
                        "1\n"
                    ]
                }
            ],
            "source": [
                "print(len(text_list))\n",
                "print(len(image_list))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "091e0550",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pre-trained CLIP model for generating embeddings\n",
                "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "41d20512",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-process the text list and image list into tensors for the model\n",
                "inputs = processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True, max_length=77, truncation=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "b3eaf452",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pass the inputs through the CLIP model to generate text and image embeddings\n",
                "outputs = model(**inputs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "93200dad",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract and store the text and image embeddings from the model output\n",
                "text_embeddings = outputs.text_embeds\n",
                "image_embeddings = outputs.image_embeds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "fd76d7ae",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([14, 512])\n",
                        "torch.Size([1, 512])\n"
                    ]
                }
            ],
            "source": [
                "print(text_embeddings.shape)\n",
                "print(image_embeddings.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "90bc80db",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the extracted text and image content lists to JSON files\n",
                "save_to_json(text_content_list, output_file='../data/processed_files/JSON/text_content.json')\n",
                "save_to_json(image_content_list, output_file='../data/processed_files/JSON/image_content.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "2f667c1a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the generated text and image embeddings to .pt files for later use\n",
                "save(text_embeddings, '../data/processed_files/embeddings/text_embeddings.pt')\n",
                "save(image_embeddings, '../data/processed_files/embeddings/image_embeddings.pt')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}